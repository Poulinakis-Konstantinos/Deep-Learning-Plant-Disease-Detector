{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plant Disease MobileNetV2 .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3cDPewXB+jxKnhh+SgUMg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Poulinakis-Konstantinos/Leaf-Disease-ML-detector-/blob/main/Plant_Disease_MobileNetV2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7cK2oW8mr_k"
      },
      "source": [
        "  Code for creating & Training a N.N. model with transfer learning based on MobileNetV2 . \n",
        "\n",
        "  The purpose of of the model is to classify plant's leaf images and extract decisions regarding their health. \n",
        "\n",
        "  It is able to classify 38 different classes ,13 different plants and some diseases that harm them . \n",
        "\n",
        "  The Model is on it's own very light , but we are still going to convert it into a TF Lite version  in order to run inference on a portable device , a RaspBerry Pi 4B . "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlmLBbVuyCD-"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHTJtv3et0yn"
      },
      "source": [
        "####### SCRIPT TO DOWNLOAD KAGGLE DATASET IN GOOGLE COLAB ######### .\n",
        "from google.colab import files\n",
        "#Upload your kaggle.json file \n",
        "files.upload() \n",
        "\n",
        "!mkdir -p  ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "#Copy the API command from kaggle .\n",
        "!kaggle datasets download -d vipoooool/new-plant-diseases-dataset\n",
        "\n",
        "#Unzip dataset ,always with -q (quiet)\n",
        "!unzip -q /content/new-plant-diseases-dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qELWBeYVfZQJ",
        "outputId": "c87874bc-c6ac-4561-c641-1d1f4b6b72b8"
      },
      "source": [
        "#Copy the API command from kaggle .\r\n",
        "!kaggle datasets download -d vbookshelf/v2-plant-seedlings-dataset\r\n",
        "\r\n",
        "#Unzip dataset ,always with -q (quiet)\r\n",
        "!unzip -q /content/v2-plant-seedlings-dataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading v2-plant-seedlings-dataset.zip to /content\n",
            "100% 3.19G/3.19G [00:57<00:00, 67.0MB/s]\n",
            "100% 3.19G/3.19G [00:57<00:00, 59.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W14LSIAClw5"
      },
      "source": [
        "image_size =224\n",
        "batch_size =32\n",
        "base_dir=\"/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_LrC6IwFZuh"
      },
      "source": [
        "class_names=[\"Apple___Apple_scab\",\"Apple___Black_rot\",\"Apple___Cedar_apple_rust\",\"Apple___healthy\",\n",
        "             \"Blueberry___healthy\",\"Cherry_(including_sour)__Powedery_mildew\",\"Cherry_(including_sour)__healthy\",\n",
        "             \"Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\",\"Corn_(maize)___Common_rust_\",\"Corn_(maize)___Northern_Leaf_Blight\",\n",
        "             \"Corn_(maize)___healthy\",\"Grape___Black_rot\",\"Grape___Esca_(Black_Measles)\",\"Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\",\n",
        "             \"Grape___healthy\",\"Orange___Haunglongbing_(Citrus_greening)\",\"Peach___Bacterial_spot\",\"Peach___healthy\",\n",
        "             \"Pepper,_bell___Bacterial_spot\",\"Pepper,_bell___healthy\",\"Potato___Early_blight\",\"Potato___Late_blight\",\n",
        "             \"Potato___healthy\",\"Raspberry___healthy\",\"Soybean___healthy\",\"Squash___Powdery_mildew\",\n",
        "             \"Strawberry___Leaf_scorch\",\"Strawberry___Healthy\",\"Tomato___Bacterial_spot\",\"Tomato___Early_blight\",\"Tomato___Late_blight\",\n",
        "             \"Tomato___Leaf_Mold\",\"Tomato___Septoria_leaf_spot\",\"Tomato___Spider_mites Two-spotted_spider_mite\",\n",
        "             \"Tomato___Target_Spot\",\"Tomato_Yellow_Leaf_Curl_Virus\",\"Tomato_mosaic_virus\",\"Tomato___healthy\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nca7eFJCDe_R",
        "outputId": "70b7c6a1-a75a-4bb7-d610-cd24ef25bc8a"
      },
      "source": [
        "# Train data set preparation, We apply data augmentation on-the-fly\n",
        "#to increase generalizability of the model.\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1/255,\n",
        "                                                            shear_range = 0.2,\n",
        "                                                            zoom_range = 0.2,\n",
        "                                                            width_shift_range = 0.2,\n",
        "                                                            height_shift_range = 0.2,\n",
        "                                                            fill_mode=\"nearest\")\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(os.path.join(base_dir,\"train\"),\n",
        "                                               target_size=(image_size,image_size),\n",
        "                                               batch_size=batch_size,\n",
        "                                               class_mode=\"categorical\" \n",
        "                                               #save_to_dir='/content/aug_pic'                                        \n",
        "                                              )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 70295 images belonging to 38 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aZie5x7FUOS",
        "outputId": "497ba3cd-f794-4b63-f109-6c3aa4caf931"
      },
      "source": [
        "# Validation data set preparation\n",
        "valid_datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1/255)\n",
        "valid_data = valid_datagen.flow_from_directory(os.path.join(base_dir,\"valid\"),\n",
        "                                               target_size=(image_size,image_size),\n",
        "                                               batch_size=batch_size,\n",
        "                                               class_mode=\"categorical\"                                              \n",
        "                                              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17572 images belonging to 38 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d-xPccYHcRD"
      },
      "source": [
        "########### CREATING THE NN MODEL (TRANSFER LEARNING) ############\n",
        "from tensorflow.keras.applications import MobileNetV2 #14 MB acc Top1 0.71 top5 0.9\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "# create the base pre-trained model . Load pretrained weights + chop off head of the network(classifier) . \n",
        "base_model_MobileNetV2 = MobileNetV2(weights='imagenet',\n",
        "                               include_top=False ,input_shape=(224,224,3))\n",
        "\n",
        "def create_model(base_model):\n",
        "      # add a global spatial average pooling layer\n",
        "      x = base_model.output\n",
        "      x = GlobalAveragePooling2D()(x)\n",
        "      # let's add a fully-connected layer\n",
        "      x = Dense(1024, activation='relu')(x)\n",
        "      # and a logistic layer -- let's say we have #number classes\n",
        "      predictions = Dense(38, activation='softmax')(x)\n",
        "\n",
        "      # this is the model we will train\n",
        "      model = Model(inputs=base_model.input, outputs=predictions \n",
        "                                      ,name=\"Plant_Disease_Detector\")\n",
        "\n",
        "      # first: train only the top layers (which were randomly initialized)\n",
        "      # i.e. freeze all convolutional MobileNet layers\n",
        "      for layer in base_model.layers:\n",
        "          layer.trainable = False\n",
        "\n",
        "      base_learning_rate = 0.0001\n",
        "      # compile the model (should be done *after* setting layers to non-trainable)\n",
        "      model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate)\n",
        "                    , loss='categorical_crossentropy' ,metrics='accuracy')\n",
        "      \n",
        "      return model\n",
        "      \n",
        "model_M = create_model(base_model_MobileNetV2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4S9GsGaqn7y"
      },
      "source": [
        "model_M.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywWvIHeypzi9"
      },
      "source": [
        "########   TRAIN   ##########\n",
        "  ##MOBILEV2 net##\n",
        "history=model_M.fit(x=train_data ,batch_size=batch_size,\n",
        "          epochs=30 ,verbose=1 ,validation_data=valid_data,\n",
        "           steps_per_epoch=150 ,validation_steps=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HcmPr-Qra-4"
      },
      "source": [
        "# at this point, the top layers are well trained and we can start fine-tuning\n",
        "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
        "# and train the remaining top layers.\n",
        "\n",
        "# let's visualize layer names and layer indices to see how many layers\n",
        "# we should freeze:\n",
        "for i, layer in enumerate(base_model_MobileNetV2.layers):\n",
        "   print(i, layer.name)\n",
        "\n",
        "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
        "# the first 249 layers and unfreeze the rest:\n",
        "for layer in model_M.layers[:249]:\n",
        "   layer.trainable = False\n",
        "for layer in model_M.layers[249:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "# we need to recompile the model for these modifications to take effect\n",
        "# we use SGD with a low learning rate\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "model_M.compile(optimizer=SGD(lr=0.0001, momentum=0.9), \n",
        "                metrics='accuracy',loss='categorical_crossentropy')\n",
        "\n",
        "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
        "# alongside the top Dense layers\n",
        "history_M2=model_M.fit(x=train_data ,batch_size=batch_size,\n",
        "          epochs=10 ,verbose=1 ,validation_data=valid_data,\n",
        "           steps_per_epoch=150 ,validation_steps=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxyECzCj7FJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5470c791-836a-4fd9-912e-129156250e6f"
      },
      "source": [
        "###### CONVERT THE MODEL #######\n",
        "###  Converting the model to it's tflite equivalent will allow\n",
        "###  it to run efficiently on a Rasberry Pi for live detection . \n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_M)\n",
        "\n",
        "#converter.optimizations = [tf.lite.Optimize.DEFAULT] ##Quantized\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model.tfliteQuant', 'wb') as f:\n",
        "  f.write(tflite_model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpmn12orh7/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5ZaQJ6FeplbK",
        "outputId": "cb296819-7928-4a12-d566-1a05fcb0787c"
      },
      "source": [
        "####### MANUAL WEIGHT SAVE #######                    ### For my expirementations\n",
        "model_M.save_weights(\"./MobileNetV2_Weights.h5\")\n",
        "files.download(\"./MobileNetV2_Weights.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_762a99c1-b231-4168-ab33-0d535a7887d7\", \"MobileNetV2_Weights.h5\", 17337576)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S7x1cY8a3Kq"
      },
      "source": [
        "####### RESTORING MODEL'S WEIGHTS ###########          ### For my expirementations\n",
        "model_M.load_weights('./MobileNetV2_Weights.h5')\n",
        "# Evaluate the model\n",
        "##model_clone.evaluate(valid_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHILUEzxbg7U"
      },
      "source": [
        "############  SAVING THE WHOLE MODEL  ###########        ### For my expirementations\n",
        "!mkdir -p saved_model \n",
        "model_M.save('saved_model/MobileNetV2') \n",
        "\n",
        "# my_model directory\n",
        "!ls saved_model\n",
        "# Contains an assets folder, saved_model.pb, and variables folder.\n",
        "!ls saved_model/MobileNetV2\n",
        "# Zip it before download (So that you can download the whole directory at once )\n",
        "!zip  -r /content/MobileNetV2_In_Zip.zip /content/saved_model\n",
        "print(\"Zip Ready\")\n",
        "# Download it \n",
        "files.download('/content/MobileNetV2_In_Zip.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF4cvt-wdg6G"
      },
      "source": [
        "############  RESTORING THE WHOLE MODEL ###########         ### For my expirementations\n",
        "files.upload()\n",
        "\n",
        "!unzip -q /content/saved_model_In_Zip.zip \n",
        "new_model = tf.keras.models.load_model('/content/content/saved_model/MobileNetV2')\n",
        "# Check its architecture\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XaVoTZ7xFfw"
      },
      "source": [
        "######  PLOTTING TRAINING INFO ########\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "plt.plot(epochs,loss,c=\"red\",label=\"Training Loss\")\n",
        "plt.plot(epochs,val_loss,c=\"blue\",label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "plt.plot(epochs,acc,c=\"red\",label=\"Training Acc\")\n",
        "plt.plot(epochs,val_acc,c=\"blue\",label=\"Validation Acc\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQc75Bz5GDwr"
      },
      "source": [
        "####### SCRIPT TO MAKE PREDICTIONS ON THE TEST SET DIRECTORY ######\n",
        "\n",
        "## I have written this script in order to automate my model evaluation on a test dataset .  \n",
        "\n",
        "import pathlib\n",
        "path='/content/test/test'\n",
        "i=0\n",
        "#Inversing class dictionary [keys,values]->[values,keys]\n",
        "labels = (train_data.class_indices)\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "\n",
        "for filename in os.listdir(path) :\n",
        "    i+=1\n",
        "    file_path= path +\"/\" + str(pathlib.Path(filename))\n",
        "   \n",
        "    image = tf.keras.preprocessing.image.load_img(\n",
        "            file_path,grayscale=False, color_mode=\"rgb\",\n",
        "            target_size=(image_size,image_size), interpolation=\"nearest\")\n",
        "    input_arr = keras.preprocessing.image.img_to_array(image)\n",
        "    input_arr=input_arr/255\n",
        "    input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
        "    prediction = model_M.predict(input_arr)\n",
        "\n",
        "\n",
        "    a=np.argmax(prediction)\n",
        "    results = labels[a] \n",
        "\n",
        "    \n",
        "    print(\"\\n\")\n",
        "   # print(\"Predicted class number :\", np.argmax(prediction))\n",
        "    print(\"Actual class :\" ,pathlib.Path(filename))\n",
        "    print(\"predicted class name->  MobileV2:\",results )\n",
        "    \n",
        "\n",
        "print(\"\\n Tested: \" ,i, \"new unseen images\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3YDoIN3PwlL",
        "outputId": "a75b02f4-c255-41d6-bbf3-f8a1a203c4c5"
      },
      "source": [
        "eval=new_model.evaluate(valid_data)\n",
        "print(eval)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "550/550 [==============================] - 60s 96ms/step - loss: 0.2129 - accuracy: 0.9318\n",
            "[0.21291173994541168, 0.9317664504051208]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}